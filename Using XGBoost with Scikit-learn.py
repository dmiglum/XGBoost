

# Importing packages
import numpy as np
from scipy.stats import uniform, randint
from sklearn.datasets import load_breast_cancer, load_diabetes, load_wine
from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error
from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split
import xgboost as xgb

# helper functions
def display_scores(scores):
    print("Scores: {0}\nMean: {1:.3f}\nStd: {2:.3f}".format(scores, np.mean(scores), np.std(scores)))

def report_best_scores(results, n_top=3):
    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results['rank_test_score'] == i)
        for candidate in candidates:
            print("Model with rank: {0}".format(i))
            print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
                  results['mean_test_score'][candidate],
                  results['std_test_score'][candidate]))
            print("Parameters: {0}".format(results['params'][candidate]))
            print("")

### Regression ###
diabetes = load_diabetes() #loading dataset

X = diabetes.data
y = diabetes.target

# Instantiating XGBoost regressor object
xgb_model = xgb.XGBRegressor(objective="reg:linear", random_state=42)

# fitting regressor on the training set and making predictions on the test set
xgb_model.fit(X, y)
y_pred = xgb_model.predict(X)

# computing mse
mse=mean_squared_error(y, y_pred)
print(np.sqrt(mse))


#### Binary classification ###
cancer = load_breast_cancer()

X = cancer.data
y = cancer.target

# Instantiating XGBoost regressor object
xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)

# fitting regressor on the training set and making predictions on the test set
xgb_model.fit(X, y)
y_pred = xgb_model.predict(X)

# printing confusion matrix
print(confusion_matrix(y, y_pred))


#### Multiclass classification ###
wine = load_wine()

X = wine.data
y = wine.target

xgb_model = xgb.XGBClassifier(objective="multi:softprob", random_state=42)

xgb_model.fit(X, y)
y_pred = xgb_model.predict(X)

# printing confusion matrix
print(confusion_matrix(y, y_pred))


### Cross validation ###
diabetes = load_diabetes()

X = diabetes.data
y = diabetes.target

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

scores = []

for train_index, test_index in kfold.split(X):   
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    xgb_model = xgb.XGBRegressor(objective="reg:linear")
    
    xgb_model.fit(X_train, y_train)    
    y_pred = xgb_model.predict(X_test)
    
    scores.append(mean_squared_error(y_test, y_pred))
    
display_scores(np.sqrt(scores))

# cross-validation using cross_val_score
xgb_model = xgb.XGBRegressor(objective="reg:linear", random_state=42)
scores = cross_val_score(xgb_model, X, y, scoring="neg_mean_squared_error", cv=5)

display_scores(np.sqrt(-scores))
    

### Hyperparameter Searching ###
diabetes = load_diabetes()

X = diabetes.data
y = diabetes.target

xgb_model = xgb.XGBRegressor()

params = {
    "colsample_bytree": uniform(0.7, 0.3),
    "gamma": uniform(0, 0.5),
    "learning_rate": uniform(0.03, 0.3), # default 0.1 
    "max_depth": randint(2, 6), # default 3
    "n_estimators": randint(100, 150), # default 100
    "subsample": uniform(0.6, 0.4)
}

search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, 
                            cv=3, verbose=1, n_jobs=1, return_train_score=True)
search.fit(X, y) #takes a little bit of time to run

report_best_scores(search.cv_results_, 2)

### Early stopping ###
#The number of boosted trees (n_estimators) to train is uncapped, rather training continues 
#until validation has not improved in n rounds
cancer = load_breast_cancer()

X = cancer.data
y = cancer.target

# if more than one evaluation metric are given the last one is used for early stopping
xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42, eval_metric="auc")

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)])
y_pred = xgb_model.predict(X_test)

accuracy_score(y_test, y_pred)
print("best score: {0}, best iteration: {1}, best ntree limit {2}".format(xgb_model.best_score, xgb_model.best_iteration, xgb_model.best_ntree_limit))


### Evaluations ###
cancer = load_breast_cancer()

X = cancer.data
y = cancer.target

xgb_model = xgb.XGBClassifier(objective="binary:logistic", n_estimators=20, random_state=42, eval_metric=["auc", "error", "error@0.6"])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])

y_pred = xgb_model.predict(X_test)


### Plotting ###
import graphviz

cancer = load_breast_cancer()

X = cancer.data
y = cancer.target

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42, eval_metric="auc")

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
xgb_model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_test, y_test)], verbose=False)

xgb.plot_importance(xgb_model)  # plotting feature importance



# plotting the output tree via matplotlib, specifying the ordinal number of the target tree
xgb.plot_tree(xgb_model, num_trees=xgb_model.best_iteration) 
# converting the target tree to a graphviz instance
xgb.to_graphviz(xgb_model, num_trees=xgb_model.best_iteration) 